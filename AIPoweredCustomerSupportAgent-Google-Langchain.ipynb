!pip install langchain langchain-google-genai langchain-community google-generativeai python-dotenv faiss-gpu faiss-cpu
from google.colab import userdata
GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
from langchain_google_genai import ChatGoogleGenerativeAI
import os

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0,
    google_api_key=GEMINI_API_KEY
)
import json
from langchain_core.prompts import ChatPromptTemplate

CLASSIFIER_PROMPT = ChatPromptTemplate.from_template("""
You are a customer support email classifier.

Email:
{email}

Return ONLY valid JSON:
{{
  "urgency": "Low | Medium | High",
  "topic": "Account | Billing | Bug | Feature Request | Technical Issue"
}}
""")

def classify_email(email: str) -> dict:
    response = llm.invoke(
        CLASSIFIER_PROMPT.format_messages(email=email)
    )

    try:
        return json.loads(response.content)
    except json.JSONDecodeError:
        # Safe fallback
        return {"urgency": "Medium", "topic": "Technical Issue"}
!pip install faiss-cpu
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

documents = [
    Document(page_content="Reset password via Settings → Security → Reset Password."),
    Document(page_content="Double billing requires finance review and refund processing."),
    Document(page_content="PDF export crash is a known bug under investigation."),
    Document(page_content="Dark mode is planned but not yet released."),
    Document(page_content="504 errors indicate backend gateway timeout issues.")
]

embeddings = GoogleGenerativeAIEmbeddings(
    model="gemini-embedding-001",
    google_api_key=GEMINI_API_KEY
)

vectorstore = FAISS.from_documents(documents, embeddings)
retriever = vectorstore.as_retriever(search_k=2)

def search_kb(query: str) -> str | None:
    results = retriever.invoke(query)
    return results[0].page_content if results else None
from langchain_core.prompts import ChatPromptTemplate

RESPONSE_PROMPT = ChatPromptTemplate.from_template("""
You are a professional customer support agent.

Customer email:
{email}

Relevant documentation:
{context}

Write a polite, clear, professional response.
Do not promise timelines.
""")

def generate_response(email: str, context: str | None) -> str:
    response = llm.invoke(
        RESPONSE_PROMPT.format_messages(
            email=email,
            context=context or "No relevant documentation found."
        )
    )
    return response.content
def should_escalate(urgency: str, topic: str, kb_result: str | None) -> bool:
    if urgency == "High":
        return True
    if topic in ["Billing", "Technical Issue"] and not kb_result:
        return True
    if topic == "Bug":
        return True
    return False
def determine_followup(topic: str, escalated: bool) -> str:
    if not escalated:
        return "None"

    if topic == "Billing":
        return "Follow up after finance review"
    if topic == "Bug":
        return "Follow up after engineering fix"
    if topic == "Technical Issue":
        return "Follow up after backend investigation"

    return "None"
def process_email(email: str) -> dict:
    classification = classify_email(email)
    urgency = classification["urgency"]
    topic = classification["topic"]

    kb_context = search_kb(email)
    response = generate_response(email, kb_context)

    escalated = should_escalate(urgency, topic, kb_context)
    followup = determine_followup(topic, escalated)

    return {
        "urgency": urgency,
        "topic": topic,
        "response_draft": response,
        "resolution_decision": "Escalate to human" if escalated else "Auto-reply",
        "follow_up_action": followup
    }

if __name__ == "__main__":
    emails = [
        "How do I reset my password?",
        "I was charged twice for my subscription!",
        "Our API integration fails with 504 errors."
    ]

    for email in emails:
        print("=" * 80)
        print(process_email(email))
